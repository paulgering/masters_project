---
title: "**Analyses for individual differences study**"
output:
  html_document: default
---

## Loading packages

```{r loadlib, echo = TRUE, results = 'hide', warning = FALSE, message = FALSE}
#renv - Store packages and keep account of package versions
if (!require('renv')) 
{
  install.packages('renv');
}
library(renv) 
renv::restore()

# Load packages
library(AER) #conduct an overdispersion test on Poisson Models
library(dplyr) #Preparing the data for analysis
library(here) #locate files based on current working directory
library(MASS) #Fit negative binomial regression
library(MuMIn) #Calculate the Akaike Information Criterion (AIC) for each model
library(WebPower) #Conduct power analysis
library(psych) #Conduct the reliability check
library(iccCounts) #Conduct the reliability check
library(tidyverse)
library(pastecs)
```

## Load the data 
Loading the data from the current working directory.

```{r}
df <- read.csv(here("raw", "SCC_ID_data_UNBLINDED_stage1.csv"))
stat.desc(df)
```

## Prospective Power Analysis

As the baselinerate, the median of infant vocab count will be used to ensure that we have an accurate calculation of the sample size that would have been needed to detect a small/medium effect size. 

```{r}
baselinerate = median(df$seventeen_cdi_vocab_count, na.rm = TRUE) #Baseline rate of infant vocab size

#Small effect size

d=0.2

lo=d*(pi/sqrt(3)) 

increase = exp(lo)

slope=(baselinerate+increase)/baselinerate #Calculating the expected slope based on an effect size of 0.2

wp.poisson(n = NULL, exp0 = baselinerate, exp1 = slope, alpha = 0.05, power = 0.8, family = "Bernoulli", alternative="greater")

#Medium effect size

d=0.5

lo=d*(pi/sqrt(3)) 

increase = exp(lo)

slope=(baselinerate+increase)/baselinerate # Calculating the expected slope based on an effect size of 0.5

wp.poisson(n = NULL, exp0 = baselinerate, exp1 = slope, alpha = 0.05, power = 0.8, family = "Bernoulli", alternative="greater")

```

## Preparing the researcher-coded behaviours 

The below code is used to prepare the counts of researcher-coded behaviours prior to incorporating them into Poisson regression models. The code takes the columns that begin with "twelve_", which are the columns containing the counts of researcher-coded behaviours (aside from twelve_length), and mutates them using the following steps: 
1. Adding one to the counts to avoid zero counts
2. Prorating the counts by dividing them by the length of the videos at 12 months and multiplying them by 300 (the maximum length of a video)
3. Log transforming the counts to reduce skewness
4. Robust scaling by subtracting the median from each of the values and dividing by the interquartile range of each column 

```{r}
# Proscaling and log transforming the counts
df1 <- df %>%
  mutate(across(
    starts_with("twelve_") & !matches("twelve_length"),
    ~ (log(((. + 1) / twelve_length) * 300))
  ))

# robust scaling to be done with the log transformed variables 
df2 <- df1 %>% 
  mutate(across(
    starts_with("twelve_") & !matches('twelve_length'),
    ~ ((. - median(.)) / IQR(.))
  ))

```

## Centering age around the mean

```{r}
# Centering age around the mean 
df2$centered_age <- scale(df2$age_at_survey, center = TRUE, scale = FALSE)
```

## Reliability check 

```{r}
#PCA only reliability, SC responses
Coder1=data.frame(cbind(df$participant,df$twelve_sc_pca_only_1,rep(1,length(df$participant))))
names(Coder1) <- c("id","y","met")

Coder2=data.frame(cbind(df$participant,df$twelve_sc_pca_only_2,rep(2,length(df$participant))))
names(Coder2) <- c("id","y","met")

for_icc=data.frame(rbind(Coder1,Coder2))
ICC(icc_counts(for_icc,y="y",id="id",met="met",type="con"))

#PCA only reliability, CL responses
Coder1=data.frame(cbind(df$participant,df$twelve_cl_pca_only_1,rep(1,length(df$participant))))
names(Coder1) <- c("id","y","met")

Coder2=data.frame(cbind(df$participant,df$twelve_cl_pca_only_2,rep(2,length(df$participant))))
names(Coder2) <- c("id","y","met")

for_icc=data.frame(rbind(Coder1,Coder2))
ICC(icc_counts(for_icc,y="y",id="id",met="met",type="con"))

```



## Fitting the models to a Poisson Distribution
Building the models using poisson regressions. 

### Model 0 - Null Model 

```{r}
M0 = glm(seventeen_cdi_vocab_count ~ condition + centered_age, family = poisson, data=df2)
```

### Model 1 - Count of infant 'intentional' Prelinguistic Communicative Acts

```{r}
M1 = glm(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_pca_count ,family=poisson,data=df2)

summary(M1)
```

### Model 2 - Count of infant 'intentional' Prelinguistic Communicative Acts that elicited a response of any kind

```{r}
M2 = glm(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_responses_pca_only_count ,family=poisson,data=df2)

summary(M2)
```

### Model 3 - Count of infant 'intentional Prelinguistic Communicative Acts that elicited a content-light response

```{r}
M3 = glm(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_cl_pca_only_count ,family=poisson,data=df2)

summary(M3)
```

### Model 4 - Count of infant 'intentional Prelinguistic Communicative Acts that elicited a semantically contingent response

```{r}
M4 = glm(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_sc_pca_only_count ,family=poisson,data=df2)


summary(M4)
```

### Model 5 - A count of (infant 'intentional Prelinguistic Communicative Acts + non-gaze-coordinated vocalisations) that elicited a response of any kind

```{r}
M5 = glm(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_responses_all_count ,family=poisson,data=df2)

summary(M5)
```

### Model 6 - A count of (infant 'intentional Prelinguistic Communicative Acts + non-gaze-coordinated vocalisations) that elicited a content-light response

```{r}
M6 = glm(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_cl_all_count ,family=poisson,data=df2)

summary(M6)
```

### Model 7 - A count of (infant 'intentional Prelinguistic Communicative Acts + non-gaze-coordinated vocalisations) that elicited a semantically contingent response

```{r}
M7 = glm(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_sc_all_count ,family=poisson,data=df2)

summary(M7)
```

## Testing for overdispersion
Determine whether overdispersion has occurred for any of the built models using the dispersiontest function.

```{r}
# Testing for overdispersion in each of the models
dispersiontest(M1)
dispersiontest(M2)
dispersiontest(M3)
dispersiontest(M4)
dispersiontest(M5)
dispersiontest(M6)
dispersiontest(M7)
```

## fitting the models to a Negative Binomial regression
If any of the dispersion tests has p < 0.05, a negative binomial regression will be fitted instead and this should be reported instead.

```{r}
M0_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age,data=df2)

summary(M0_nb)
```

```{r}
M1_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_pca_count,data=df2)

summary(M1_nb)
```

```{r}
M2_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_responses_pca_only_count,data=df2)

summary(M2_nb)
```

```{r}
M3_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_cl_pca_only_count,data=df2)

summary(M3_nb)
```

```{r}
M4_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_sc_pca_only_count,data=df2)


summary(M4_nb)
```

```{r}
M5_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_responses_all_count,data=df2)

summary(M5_nb)
```

```{r}
M6_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_cl_all_count,data=df2)

summary(M6_nb)
```

```{r}
M7_nb = glm.nb(seventeen_cdi_vocab_count ~ condition + centered_age + twelve_sc_all_count,data=df2)

summary(M7_nb)
```

## Testing Prediction 1: Infants who produce more PCA's will have a larger vocabulary size at 18 months 

```{r}
anova(M1, M0)
```

If the tests detect overdispersion for any of the models, the models built using a negative binomial regression will be included in this code instead of the models built using a Poisson Regression. 

If overdispersion is detected for model 1 and the null model: 

```{r}
anova(M1_nb, M0_nb)
```

## Comparing the AIC of the models to test predictions 2, 3, and 4 and to answer two questions in an exploratory analysis:

```{r}
model.sel(M1, M2, M3, M4, M5, M6, M7)
```

Alternative code in case overdispersion is detected in all of the models: 

```{r}
model.sel(M1_nb, M2_nb, M3_nb, M4_nb, M5_nb, M6_nb, M7_nb)
```

## Comparing the fit of model 2 and the null model 
Given that model 2 was a marginal improvement in fit on model 1, it was decided that the fit of model 2 would be compared to the null model to determine whether including infant PCAs + response in the model significantly improved the amount of variance explained in infant vocabulary size. 

```{r}
anova(M2_nb, M0_nb)
```

## Preparing the data for visualisation

```{r}
# Model 1 - infant PCAs
CIs=confint(M1_nb)
vocab_median_pca=exp(M1_nb$coefficients[1])
vocab_1_IQR_above_median_pca = exp(M1_nb$coefficients[1]+M1_nb$coefficients[4])
vocab_1_IQR_above_median_pca_lb = exp(M1_nb$coefficients[1]+CIs[4])
vocab_1_IQR_above_median_pca_ub = exp(M1_nb$coefficients[1]+CIs[8])

# Model 2 - infant PCAs with response
CIs=confint(M2_nb)
vocab_median_responses_pca=exp(M2_nb$coefficients[1])
vocab_1_IQR_above_median_responses_pca = exp(M2_nb$coefficients[1]+M2_nb$coefficients[4])
vocab_1_IQR_above_median_responses_pca_lb = exp(M2_nb$coefficients[1]+CIs[4])
vocab_1_IQR_above_median_responses_pca_ub = exp(M2_nb$coefficients[1]+CIs[8])

# Model 3 - infant PCAs with content-light
CIs=confint(M3_nb)
vocab_median_cl_pca=exp(M3_nb$coefficients[1])
vocab_1_IQR_above_median_cl_pca = exp(M3_nb$coefficients[1]+M3_nb$coefficients[4])
vocab_1_IQR_above_median_cl_pca_lb = exp(M3_nb$coefficients[1]+CIs[4])
vocab_1_IQR_above_median_cl_pca_ub = exp(M3_nb$coefficients[1]+CIs[8])

# Model 4 - infant PCAs with semantically contingent response
CIs=confint(M4_nb)
vocab_median_sc_pca=exp(M4_nb$coefficients[1])
vocab_1_IQR_above_median_sc_pca = exp(M4_nb$coefficients[1]+M4_nb$coefficients[4])
vocab_1_IQR_above_median_sc_pca_lb = exp(M4_nb$coefficients[1]+CIs[4])
vocab_1_IQR_above_median_sc_pca_ub = exp(M4_nb$coefficients[1]+CIs[8])

# Model 5 - any infant behaviour with response
CIs=confint(M5_nb)
vocab_median_responses_all=exp(M5_nb$coefficients[1])
vocab_1_IQR_above_median_responses_all = exp(M5_nb$coefficients[1]+M5_nb$coefficients[4])
vocab_1_IQR_above_median_responses_all_lb = exp(M5_nb$coefficients[1]+CIs[4])
vocab_1_IQR_above_median_responses_all_ub = exp(M5_nb$coefficients[1]+CIs[8])

# Model 6 - any infant behaviour with content-light
CIs=confint(M2_nb)
vocab_median_cl_all=exp(M6_nb$coefficients[1])
vocab_1_IQR_above_median_cl_all = exp(M6_nb$coefficients[1]+M6_nb$coefficients[4])
vocab_1_IQR_above_median_cl_all_lb = exp(M6_nb$coefficients[1]+CIs[4])
vocab_1_IQR_above_median_cl_all_ub = exp(M6_nb$coefficients[1]+CIs[8])

# Model 7 - any infant behaviour with semantically contingent response
CIs=confint(M7_nb)
vocab_median_sc_all=exp(M7_nb$coefficients[1])
vocab_1_IQR_above_median_sc_all = exp(M7_nb$coefficients[1]+M7_nb$coefficients[4])
vocab_1_IQR_above_median_sc_all_lb = exp(M7_nb$coefficients[1]+CIs[4])
vocab_1_IQR_above_median_sc_all_ub = exp(M7_nb$coefficients[1]+CIs[8])

my_table <- data.frame(
  Column1 = c("PCA (M1)", "PCA + response (M2)", "PCA + CL response (M3)", "PCA + SC response (M4)", "PCA + vocs + response (M5)", "PCA + vocs + CL response (M6)", "PCA + vocs + SL response (M7)"), 
  Column2 = c(vocab_median_pca, vocab_median_responses_pca, vocab_median_cl_pca, vocab_median_sc_pca, vocab_median_responses_all, vocab_median_cl_all, vocab_median_sc_all), 
  Column3 = c(vocab_1_IQR_above_median_pca, vocab_1_IQR_above_median_responses_pca, vocab_1_IQR_above_median_cl_pca, vocab_1_IQR_above_median_sc_pca, vocab_1_IQR_above_median_responses_all, vocab_1_IQR_above_median_cl_all, vocab_1_IQR_above_median_sc_all), 
  Column4 = c(vocab_1_IQR_above_median_pca_lb, vocab_1_IQR_above_median_responses_pca_lb, vocab_1_IQR_above_median_cl_pca_lb, vocab_1_IQR_above_median_sc_pca_lb, vocab_1_IQR_above_median_responses_all_lb, vocab_1_IQR_above_median_cl_all_lb, vocab_1_IQR_above_median_sc_all_lb),
  Column5 = c(vocab_1_IQR_above_median_pca_ub, vocab_1_IQR_above_median_responses_pca_ub, vocab_1_IQR_above_median_cl_pca_ub, vocab_1_IQR_above_median_sc_pca_ub, vocab_1_IQR_above_median_responses_all_ub, vocab_1_IQR_above_median_cl_all_ub, vocab_1_IQR_above_median_sc_all_ub)
)

colnames(my_table) <- c("variable", "median", "IQR_above_median", "lower", "upper")

my_table1 <- my_table %>%
  mutate(
    IQR_above_median = IQR_above_median - median, 
    lower = lower - median, 
    upper = upper - median
  )

str(my_table1)

p <- ggplot(data = my_table1, aes(x = reorder(variable, IQR_above_median),
                              y = IQR_above_median, ymin = lower, ymax = upper)) + scale_y_continuous(breaks = seq(-20, 70, by = 10)) +
  geom_hline(yintercept = 0, linetype = "dotted", color = "black") +
  geom_pointrange() + 
  coord_flip() + 
  labs(x = "Caregiver-infant dyadic sequences (predictors)", y = "Change in vocabulary size per 1 SD increase from the median in the predictor", title = "The effects of caregiver-infant dyadic sequences on infant vocabulary size") + 
  theme(axis.title.x = element_text(size = 10, margin = margin(t = 15)), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "inches"), plot.title = element_text(size = 12, hjust = 0.68))

p

ggsave(here("figs", "plot.png"), plot = p)
```

